---
title: "Project 1 (F)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	eval = TRUE,
	echo = TRUE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	R.options = list(max.print = 100),
	tidy = TRUE,
	tidy.opts = list(width.cutoff = 60)
)
```


```{r}
getwd()
work_dir <- "/stor/home/jj37257/website/content/project"
Marriage <- read.csv("Marriage.csv")
weather <- read.csv("2530921.csv")

library(dplyr)
library(ggridges)
library(tidyr)
library(ggplot2)
library(kableExtra)
library(tibble)
library(cluster)
```



```{r}
## The data sets that I have chosen for this project were marriage and weather data from Mobile County, Alabama. I thought that these two data could go together when you think about how many people decide to have their ceremony based on the weather. People want to have a beautiful wedding on the perfect day, so I was wondering if there was a correlation between the weather and marriage ceremonies. Additionally, I was also curious if there was a correlation betweeen age, race, and delay of ceremony. The delay of ceremony was calculated by the number of days between the ceremony and the marriage application process. I was predicting that younger poeple would be more likely to rush into things and have the marriage ceremony soon after the application process than those who were older. As for race, I did not really have a prediction for it, but I was curious to see if there was any correlation between race and delay which could possibly be explained by socioeconomic factors. 

## The variables in the marriage dataset had the application date, the ceremony date, the delay between the two, various demographic factors of the person who applied for it as well as who officiated their marriage. I acquired this data through the fivethirtyeight package. On the other hand, I acquired the weather dataset through the NOAA site which had an abundant archive for various places in the US. The variables in this set included the average, max, and min temperature for a specific day. It also included the precipitation levels for that day, which in Mobile County was not a huge variable. 


joined_data <- Marriage %>% left_join(weather, by = c('appdate' = 'DATE')) 
 

## To merge and tidy up the two datasets, I decided to do a left join since I matched both data by the dates. First, I looked at the marriage application dates and then proceeded to find the weather reports corresponding to those dates. The left join was the most reasonable choice to do because I wanted all the data from the left table, which was the marriage table, since that dataset had more variables I was going to look into. Furthermore, I needed to narrow down the weather dates by matching it with the marriage application dates. 

 
```


```{r}
joined_data %>%  na.omit() %>% filter(!is.na(TAVG))  %>% 
  filter(between(delay,0,27)) %>% 
  arrange(desc(appdate),desc(TAVG))%>%
  select(-bookpageID, -person,-dob,-prevconc,-prevcount,-STATION, -officialTitle,-dayOfBirth, -hs, -college) %>%
  select(appdate,ceremonydate,delay,TAVG,TMAX,TMIN,PRCP,everything()) %>%
  mutate(delay_cat = case_when(delay>15 ~"high",delay<= 15 & 5<= delay ~"med", delay <5 ~ "low"))


## The only case that was dropped were some of the values in the average temperature because the data that I acquired did not have all of the daily average temperatures. Therefore, I had to drop those after joining the two, which helped me narrow down my data. There were no potential problems with this because the marriage dataset had 98 observations and since we only neeeded atleast 50, dropping a few of the data did not present a problem with the amount of data that was required for this project. However, average temperature is a crucial element of trying to determine if there is a correlation between weather and wedding delays. This aspect did present a problem in that it messed the the mean and other summary statistics of this variable. 

## After joining the datasets, I was required to use the 6 main dplyr functions. I decided to first filter the delay column of the data so that there were no huge outliers in the data. Then I proceeded to arrange the data by descending order of application date first which was then proceeded by descending order of average temperature for that day because some of the application dates for various people were similar and some of the average temperature datas were not recorded daily. Next, I used the select function to drop the unnecessary variables in the data such as date of birth, highschool/college level,station of the weather report, etc. I used the mutate function to create a new categorica variable where the delays were classified into levels of high, medium, or low. 
```

```{r}
joined_data%>%summarize(mean_delay=mean(delay, na.rm=T),sd_delay=sd(delay, na.rm=T), var_delay=var(delay, na.rm=T), max_delay=max(delay, na.rm=T), min_delay=min(delay, na.rm=T), IQR_delay=IQR(delay, na.rm=T), 
  mean_TAVG =mean(TAVG, na.rm=T),sd_TAVG=sd(TAVG, na.rm=T), var_TAVG=var(TAVG, na.rm=T),  max_TAVG=max(TAVG, na.rm=T),  min_TAVG=min(TAVG, na.rm=T),  IQR_TAVG=IQR(TAVG, na.rm=T), 
  mean_TMAX=mean(TMAX, na.rm=T),sd_TMAX=sd(TMAX, na.rm=T), var_TMAX=var(TMAX, na.rm=T), max_TMAX=max(TMAX, na.rm=T), min_TMAX=min(TMAX, na.rm=T), IQR_TMAX=IQR(TMAX, na.rm=T),
  mean_TMIN=mean(TMIN, na.rm=T),sd_TMIN=sd(TMIN, na.rm=T),var_TMIN=var(TMIN, na.rm=T),max_TMIN=max(TMIN, na.rm=T),min_TMIN=min(TMIN, na.rm=T),IQR_TMIN=IQR(TMIN, na.rm=T),
  mean_PRCP=mean(PRCP, na.rm=T),sd_PRCP=sd(PRCP, na.rm=T),var_PRCP=var(PRCP, na.rm=T),max_PRCP=max(PRCP, na.rm=T),min_PRCP=min(PRCP, na.rm=T),IQR_PRCP=IQR(PRCP, na.rm=T),
  mean_age=mean(age, na.rm=T),sd_age=sd(age, na.rm=T),var_age=var(age, na.rm=T),max_age=max(age, na.rm=T),min_age=min(age, na.rm=T),IQR_age=IQR(age, na.rm=T),
  n_rows=n()) %>% 
 pivot_longer(contains("_"))%>%  separate(name,into=c("type","numeric variable")) %>% kbl() %>% kable_styling()

## My dataset was already tidy after joining it, so I waited till after summarize() to tidy the data since it made my tables look a little messy. When using just the general summarize function by itself, I just had to tidy the data from wide to long since there were not that many variables. I used the pivot longer function to seperate my numeric variables, such as max and min temperature, delay, and percipitation, into the type of summary that was generated such as mean, sd, and var. In order to do that, I first had to seperate the name by the "_" because when summarizing I had created a new variable with that notation in it. After seoerating the name, I renamed the variables accordingly. On the other hand, when using the group by function, I had to use both pivot wider and longer because there were multiple rows and columns of each variable. It had made the dataset very long, so the most reasonable method was to combine the two pivot functions and group the variables accordingly as was done when just the summarize function was used. 
```

```{r}
joined_data %>%group_by(appdate,delay,PRCP,race,sign) %>%
  summarize(mean_TAVG =mean(TAVG, na.rm=T),sd_TAVG=sd(TAVG, na.rm=T), var_TAVG=var(TAVG, na.rm=T),  max_TAVG=max(TAVG, na.rm=T),  min_TAVG=min(TAVG, na.rm=T),  IQR_TAVG=IQR(TAVG, na.rm=T), 
  mean_TMAX=mean(TMAX, na.rm=T),sd_TMAX=sd(TMAX, na.rm=T), var_TMAX=var(TMAX, na.rm=T), max_TMAX=max(TMAX, na.rm=T), min_TMAX=min(TMAX, na.rm=T), IQR_TMAX=IQR(TMAX, na.rm=T),
  mean_TMIN=mean(TMIN, na.rm=T),sd_TMIN=sd(TMIN, na.rm=T),var_TMIN=var(TMIN, na.rm=T),max_TMIN=max(TMIN, na.rm=T),min_TMIN=min(TMIN, na.rm=T),IQR_TMIN=IQR(TMIN, na.rm=T))%>% distinct(appdate, .keep_all = TRUE)%>% na.omit() %>% 
  pivot_longer(contains("_"))%>%  separate(name,into=c("type","numeric variables")) %>% 
  pivot_wider(names_from="type",values_from="value")%>%
  kbl() %>% kable_styling()

##Summarize the procedure and discuss all (or the most interesting) results in no more than two paragraphs
## The summarize function had to be the most tedious of the ones we had to use. There were a lot of summary statistics that I wanted to generate for each of my numerical data. For this one, I also had to make sure that duplicated rows were not being repeated, so I added a distinct function in the code to remove any duplicated rows. This was the most interesting to me because the data looked clean and I was able to see all the variables that I was interested in and the major summary statistics at one place. 
```


```{r}
joined_data %>% na.omit() %>% filter(!is.na(TAVG))%>%
  select_if(is.numeric) %>% cor(use="pair") %>% 
  as.data.frame %>% rownames_to_column("var1") %>%pivot_longer(-1,names_to="var2",values_to="correlation") %>%
  ggplot(aes(var1,var2,fill=correlation))+  
    geom_tile()+  
    scale_fill_gradient2(low="red",mid="white",high="blue")+
    geom_text(aes(label=round(correlation,2)),color = "black", size = 4)+
    theme(axis.text.x = element_text(angle = 90, hjust=1))+  xlab("")+ylab("")+
    coord_fixed()

## This plot is the correlation plot and it showed the strength of association of the variables with each other. This plot is important because a value close to 1 indicates a strong correlation whereas a value close to 0 indicates poor correlation. As with the numbers, it is also important to make note of the signs of the values as that indicates the direction of correlation. A positive value indicates a upward correlation where as one variable increases so does the other. A negative value indicates that as one variable decreases, the other increases. The noteworthy trends that were apparent in this plot was that there was a strong correlation between mininum temperature and age as well as delay and age. This led me to infer that young people were more likely to have their ceremony sooner than those who were older. Furthermore, younger people were more likely to go get married when the temperature was warmer. 
```

```{r}

plot1<- ggplot(data = joined_data, aes(x = delay, y = PRCP)) +
   geom_line(aes(color = age)) + geom_point(size=2, color = "gray") +
  theme(legend.position="top")
plot1 + ggtitle("Influence of weather and age on delays in Marriage") + ylab("Rainfall (in)") + xlab("Delay (days)") 
plot1 + facet_wrap(~race)

## The second plot that I decided to do was a linear plot that the specifically tested the relationship of weather and age on delays in marriage ceremonies. As can be seen in the first plot, younger people were more likely to delay their ceremony if it was raining a certain day. Older people were more likely than younger people to delay the ceremony regardless of rainfall. This plot does not show the most substanttial data because I think more points were needed. A strong correlation between the variables could not be determined by this plot. Additionally, I decided to facet the data and look at specific race. In the second plot, it can be assumed that African Americans would have their ceremonies sooner than the other races regardless of percipitation and white people were the second race to not delay the wedding. However, an important fact to note here is that this data may not have been very proportionate when it comes to accounting for races. There were far less American Indian and Hispanic people in this data, therefore, a strong assumption cannot be made. 
```

```{r}
plot2 <- ggplot(joined_data)+ aes(x = sign, y = delay, color = age) +  geom_errorbar(stat="summary", width=.2) + scale_fill_hue(c=70)+ geom_bar(size = 2, color = "blue", stat="summary",fun= mean)
plot2 + ggtitle( "Influence of birth sign and age on marriage delays") + ylab("Delays (days)") + xlab("Sign") 


## The last plot that I decided to do to visualize my data was a histogram with standard errors. I decided to look at the influence of the variables birth sign and age on marriage delays. I was expecting to see a slight trend with this one because I was thinking that there are some people who are really into horoscopes and would be influenced by their star signs on an important day. As can be seen in the plot, there is not a normal distribution, it seems to be bimodial with more Aries and Sagitarius delaying their marriage. Capricorns were the least to delay their ceremony. The standard error bars seem to be equaly proportionate throughtout the plot. 
```

```{r}
dat2<-joined_data%>%select(-bookpageID, -person,-dob,-prevconc,-prevcount,-STATION, -officialTitle,-dayOfBirth, -hs, -college) %>%
  select(appdate,ceremonydate,delay,TAVG,TMAX,TMIN,PRCP,everything()) %>%
  mutate_if(is.character,as.factor)

gower1<-daisy(dat2,metric="gower")
pam3<-pam(gower1,k=3,diss=T)

sil_width<-vector()
for(i in 2:10){    
  pam_fit <- pam(gower1, diss = TRUE, k = i)   
  sil_width[i] <- pam_fit$silinfo$avg.width  
  }
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

pam3 <- pam(gower1, k = 2, diss = T) 
pam3

gower1%>%as.matrix%>%as.data.frame%>%rownames_to_column%>%pivot_longer(-1,values_to="distance")%>%
  filter(rowname!=name)%>%distinct(distance,.keep_all = T)%>%filter(distance%in%c(min(distance),max(distance)))

joined_data%>%slice(32,340) 
joined_data%>%slice(4,5) 

dat2%>%mutate(cluster=pam3$clustering)%>%group_by(cluster)%>%mutate(n=n())%>%summarize_at(2:8,mean,na.rm=T)


dat2 %>% mutate(cluster = pam3$clustering) %>% rename_all(function(x)sub("_",  ".",  x)) %>% 
group_by(cluster) %>% mutate(n = n()) %>% group_by(cluster, n) %>% summarize_at(3:7, .funs =  list("mean" = mean, "median" = median, "sd" = sd), na.rm = T) %>% 
pivot_longer(contains("_")) %>% separate(name, sep = "_", into = c("variable", "stat")) %>% pivot_wider(names_from =  "variable", values_from = "value") %>% arrange(stat)

plot(pam3,which=2)

pam3$silinfo$widths %>%  as.data.frame %>% mutate(x=384:1) %>%   ggplot(aes(x, y=sil_width, fill=as.factor(cluster))) + geom_bar(stat="identity") + xlab("sil_width") + 
facet_grid(cluster~., scales="free_y") + coord_flip() + theme(legend.position="none")

##Supporting paragraph or two describing results found, interpreting the clusters/PCs in terms of the original variables and observations, discussing goodness of fit or variance explained, etc.

## First, I decided to cluster using categorical variables with Gower dissimilarities. It is important to pick the right number of cluster. As can be seen by the first plot, 2 is the best because silhouette width indexes how well points are assigned to their cluster. Higher silhouette width is better because it is more separated and smaller number of clusters are more parsimonious. After figuring out the number of clusters, I used that in accordance with PAM to find which variables of my data are the most similar and the most different. The most different was between a male who had a 28 day delay and a woman who had a 0 day delay. They were complete oposites in all aspects of the data. The most similar were two grooms who had the same application and ceremony date as well as race and age similarity. After, I decided to do summary statistics for numeric variables by cluster. Through this I found that cluster 1 had greater delay than cluster 2. Then I looked for the proportions for each statistics. I found that most that the most delayed group was in cluster 1 and cluster 1 is least likely to have their ceremony on a rainy day. CLuster 2 is more likely to have their ceremony on a warmer day. It is important to not that there is an uneven distribution within the clusters thereby making the data misleading. Next, when looking at in terms of goodness of fitm the plot does not look too great. The average silhoutte width is 0.18 which means that no substantial structure has been found. 
```



